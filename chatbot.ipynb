{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x109dcd690>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x109dcc9d0>, model_name='Gemma2-9b-It', groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Abishek!\\n\\nIt's nice to meet you. That's awesome that you're a data scientist. \\n\\nWhat kind of work do you do in data science?  Are you working on any interesting projects right now? \\n\\nI'm always eager to learn more about what people are doing in this field. üòä \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 27, 'total_tokens': 102, 'completion_time': 0.156027436, 'prompt_time': 0.002606972, 'queue_time': None, 'total_time': 0.158634408}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-6e15fcd3-fbc5-491c-b5a3-29ed04d66499-0', usage_metadata={'input_tokens': 27, 'output_tokens': 75, 'total_tokens': 102})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi, my name is Abishek and i am data scientist\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You told me your name is Abishek Kumar Yadav and that you are a data scientist!  \\n\\nIs there anything else you'd like to tell me about yourself or your work? I'm happy to chat. üòä \\n\", response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 116, 'total_tokens': 166, 'completion_time': 0.102639857, 'prompt_time': 0.007855055, 'queue_time': None, 'total_time': 0.110494912}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-4931b47a-f6b7-4990-ac16-7405ef0f9bf0-0', usage_metadata={'input_tokens': 116, 'output_tokens': 50, 'total_tokens': 166})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke([\n",
    "    HumanMessage(content=\"Hi, my name is Abishek kumar yadav and I am a data scientist\"),\n",
    "    AIMessage(content=\"Hello Abishek! \\n\\nIt's nice to meet you.  \\n\\nBeing a data scientist is fascinating! What kind of work do you do? What are you most passionate about in the field?  \\n\\nI'm always eager to learn more about how people use data to make a difference. üòä\"),\n",
    "    HumanMessage(content=\"hey what's my name and what do i do?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message History\n",
    "-  We can use a message history class to wrap out model and make it statful. This will kep track of input and outputs of the model, and store them in some datastore. Futute interactions will then load those messages and pass them into the cahin as part of the input. Let's see how to use this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "store={}\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, I am abishek kumar and i am a data scientist\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Abhishek Kumar, it's nice to meet you! \\n\\nBeing a data scientist is a fascinating field. What kind of work do you specialize in?  \\n\\nI'm eager to learn more about your experience and projects.  Perhaps you could tell me about:\\n\\n* **A recent project you're proud of.**\\n* **The most challenging problem you've tackled.**\\n* **Your favorite tools or techniques in your data science toolbox.** \\n\\n\\nLet's chat! üòÑ \\n\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You told me your name is Abhishek Kumar and that you are a data scientist.  \\n\\nIs there anything else you'd like to tell me about your work or yourself?  I'm here to listen! üòä  \\n\", response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 151, 'total_tokens': 199, 'completion_time': 0.098175283, 'prompt_time': 0.008186366, 'queue_time': None, 'total_time': 0.106361649}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-de996763-a469-401a-b6e0-ca47539847b6-0', usage_metadata={'input_tokens': 151, 'output_tokens': 48, 'total_tokens': 199})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name and role i am working?\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change the session Id\n",
    "config2={\"configurable\":{\"session_id\":\"chat2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'session_id': 'chat2'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What my name?\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Abhishek Kumar.  \\n\\nI remember that from our earlier conversation!  üòä  \\n\\nIs there anything else I can help you with?\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My name is Abishek?\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're right!  My apologies, I seem to have made a small mistake. \\n\\nYour name is Abishek, not Abhishek. I'm still under development and learning to be more accurate. Thanks for pointing that out!  \\n\\nWhat can I help you with now? \\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What my name?\")],\n",
    "    config=config2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me your name, I'd be happy to use it! üòä\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, I am abishek kumar and i am a data scientist'), AIMessage(content=\"Hi Abhishek Kumar, it's nice to meet you! \\n\\nBeing a data scientist is a fascinating field. What kind of work do you specialize in?  \\n\\nI'm eager to learn more about your experience and projects.  Perhaps you could tell me about:\\n\\n* **A recent project you're proud of.**\\n* **The most challenging problem you've tackled.**\\n* **Your favorite tools or techniques in your data science toolbox.** \\n\\n\\nLet's chat! üòÑ \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 106, 'prompt_tokens': 28, 'total_tokens': 134, 'completion_time': 0.219783684, 'prompt_time': 0.002621402, 'queue_time': None, 'total_time': 0.222405086}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-bc06071f-b002-4955-8aab-695c81531582-0', usage_metadata={'input_tokens': 28, 'output_tokens': 106, 'total_tokens': 134}), HumanMessage(content='What is my name and role i am working?'), AIMessage(content=\"You told me your name is Abhishek Kumar and that you are a data scientist.  \\n\\nIs there anything else you'd like to tell me about your work or yourself?  I'm here to listen! üòä  \\n\", response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 151, 'total_tokens': 199, 'completion_time': 0.098175283, 'prompt_time': 0.008186366, 'queue_time': None, 'total_time': 0.106361649}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-de996763-a469-401a-b6e0-ca47539847b6-0', usage_metadata={'input_tokens': 151, 'output_tokens': 48, 'total_tokens': 199}), HumanMessage(content='What my name?'), AIMessage(content='Your name is Abhishek Kumar.  \\n\\nI remember that from our earlier conversation!  üòä  \\n\\nIs there anything else I can help you with?\\n', response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 211, 'total_tokens': 244, 'completion_time': 0.06744762, 'prompt_time': 0.008380707, 'queue_time': None, 'total_time': 0.075828327}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e8836eb6-5529-44eb-88fb-13178c17047f-0', usage_metadata={'input_tokens': 211, 'output_tokens': 33, 'total_tokens': 244}), HumanMessage(content='My name is Abishek?'), AIMessage(content=\"You're right!  My apologies, I seem to have made a small mistake. \\n\\nYour name is Abishek, not Abhishek. I'm still under development and learning to be more accurate. Thanks for pointing that out!  \\n\\nWhat can I help you with now? \\n\", response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 259, 'total_tokens': 322, 'completion_time': 0.130289117, 'prompt_time': 0.009908478, 'queue_time': None, 'total_time': 0.140197595}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee70e7ac-ef6e-405f-87ae-3f1751820ae8-0', usage_metadata={'input_tokens': 259, 'output_tokens': 63, 'total_tokens': 322})]),\n",
       " 'chat2': InMemoryChatMessageHistory(messages=[HumanMessage(content='What my name?'), AIMessage(content=\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me your name, I'd be happy to use it! üòä\\n\", response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 18, 'total_tokens': 60, 'completion_time': 0.086504934, 'prompt_time': 0.002369662, 'queue_time': None, 'total_time': 0.088874596}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a27ce95a-a278-4b62-99d2-3bbe65b18252-0', usage_metadata={'input_tokens': 18, 'output_tokens': 42, 'total_tokens': 60})])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.chat_history.InMemoryChatMessageHistory"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "- Prompt Templates help to turn user information into a format that the LLm can work with. In this case, the raw user input is hist a message, which are passing to the llm. Lets now make that a bit more complicated. First, Let's add in a system message with some custom intstructions (but still taking message as input) Next, we'll ass in more input besides just the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistance. Answer all the question to the nest of yout abililty\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Abishek, it's nice to meet you! üëã\\n\\nHow can I help you today? üòä \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 40, 'total_tokens': 68, 'completion_time': 0.056959223, 'prompt_time': 0.002913242, 'queue_time': None, 'total_time': 0.059872465}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-c36d9a62-632b-4c39-af2e-13665116634c-0', usage_metadata={'input_tokens': 40, 'output_tokens': 28, 'total_tokens': 68})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is Abishek\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Abishek, it's nice to meet you! üëã\\n\\nIs there anything I can help you with today? üòÑ  I'm ready for your questions!\\n\", response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 40, 'total_tokens': 78, 'completion_time': 0.078062678, 'prompt_time': 0.002905632, 'queue_time': None, 'total_time': 0.08096831}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-129f003c-54cc-4e78-a390-0dbfeca500fd-0', usage_metadata={'input_tokens': 40, 'output_tokens': 38, 'total_tokens': 78})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi my name is Abishek\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity\n",
    "\n",
    "prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistance. Answer all question to the best of your ability in language {language}\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is Abishek\")], \"language\":\"Nepali\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á Abishek! üòä  ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•ã ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡•á ‡§Ø‡§π‡§æ‡§Å ‡§Ü‡§è ‡•§ ‡§Æ ‡§∏‡§ß‡•à‡§Å ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§Æ‡§¶‡§§ ‡§ó‡§∞‡•ç‡§® ‡§§‡§Ø‡§æ‡§∞ ‡§õ‡•Å‡•§ ‡§ï‡•á ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§ï‡•á‡§π‡•Ä ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§õ‡•Å? \\n\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets now wrap this more complicated chain in a message history class. This time, because there are multiple keys in the input, we need to specity the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain, \n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat4\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    {'messages':[HumanMessage(content=\"Hi, I an Abishek\")], \"language\":\"Nepali\"},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Namaste Abhishek! üòä \\n\\n‡§Æ‡§≤‡§æ‡§à ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§™‡§∞‡§ø‡§ö‡§Ø ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§≤‡§æ‡§ó‡•ç‡§Ø‡•ã‡•§ ‡§ï‡§∏‡§∞‡•Ä ‡§õ‡•Å‡§Å ‡§§‡§™‡§æ‡§à‡§Ç ? \\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    {'messages':[HumanMessage(content=\"Whats my name\")], \"language\":\"Nepali\"},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§π‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§ï‡•Å‡§∞‡§æ‡§ï‡§æ‡§®‡•Ä‡§Æ‡§æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡•á ‡§Ü‡§´‡•ç‡§®‡•ã ‡§®‡§æ‡§Æ Abishek ‡§≠‡§®‡•ç‡§®‡•Å‡§≠‡§Ø‡•ã‡•§  \\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing the conversation History\n",
    "- One importanct concept to understand when building chatbots is how to manage converation history. If left unmanged, the list of message will grow unbounded and potentially overflow the context window of the LLM. Therefore. It is important to add a step that limits the size of the message you passing in.\n",
    "\n",
    "\"Trim Message\" helper to redue how many messages we're sending to the model. The trimer allows us to speciy how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer=trim_messages(\n",
    "    max_tokens=75, \n",
    "    strategy=\"last\", # It going to focus on the last conversation that it had.\n",
    "    token_counter=model, # Function or llm counting tokens in Bese message list.\n",
    "    include_system=True, # Include system message\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\"),\n",
       " HumanMessage(content=\"Hi! I'm Bob\"),\n",
       " AIMessage(content='hi!'),\n",
       " HumanMessage(content='I like vanilla ice cream'),\n",
       " AIMessage(content='nice'),\n",
       " HumanMessage(content='Whats 2 + 2'),\n",
       " AIMessage(content='4'),\n",
       " HumanMessage(content='Thanks'),\n",
       " AIMessage(content='No, problem'),\n",
       " HumanMessage(content='Having fun?'),\n",
       " AIMessage(content='Yes!')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages=[\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"Whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"Thanks\"),\n",
    "    AIMessage(content=\"No, problem\"),\n",
    "    HumanMessage(content=\"Having fun?\"),\n",
    "    AIMessage(content=\"Yes!\")\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You said you like vanilla ice cream! üòä  \\n', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 125, 'total_tokens': 138, 'completion_time': 0.026, 'prompt_time': 0.006419434, 'queue_time': None, 'total_time': 0.032419434}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-38d3e3be-09a5-4bdc-9145-c28be475619e-0', usage_metadata={'input_tokens': 125, 'output_tokens': 13, 'total_tokens': 138})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "chain.invoke(\n",
    "    {\n",
    "    \"messages\": messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wrap thus in the messages History.\n",
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    "\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Abishek! It's nice to meet you. üòä  \\n\\nHow are you doing today?\\n\", response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 125, 'total_tokens': 150, 'completion_time': 0.05, 'prompt_time': 0.007621135, 'queue_time': None, 'total_time': 0.057621135000000004}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-151ab2c1-78ac-4d48-b742-ec2d7b0f0d04-0', usage_metadata={'input_tokens': 125, 'output_tokens': 25, 'total_tokens': 150})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"My name is Abishek\")],\n",
    "        \"language\":\"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob. üòä  \\n\\n\\n\\n', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 122, 'total_tokens': 132, 'completion_time': 0.02, 'prompt_time': 0.009054717, 'queue_time': None, 'total_time': 0.029054717}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-73169760-ad4f-4268-a2c6-4405314f27e9-0', usage_metadata={'input_tokens': 122, 'output_tokens': 10, 'total_tokens': 132})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name\")],\n",
    "        \"language\":\"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
